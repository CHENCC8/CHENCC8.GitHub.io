{
    "version": "https://jsonfeed.org/version/1",
    "title": "Orange • All posts by \"深度学习\" category",
    "description": "",
    "home_page_url": "https://chencc8.github.io",
    "items": [
        {
            "id": "https://chencc8.github.io/2022/03/14/juan-ji-he-juan-ji-shen-jing-wang-luo/",
            "url": "https://chencc8.github.io/2022/03/14/juan-ji-he-juan-ji-shen-jing-wang-luo/",
            "title": "卷积和卷积神经网络",
            "date_published": "2022-03-14T14:30:44.000Z",
            "content_html": "<h1 id=\"卷积和卷积神经网络\"><a href=\"#卷积和卷积神经网络\" class=\"headerlink\" title=\"卷积和卷积神经网络\"></a>卷积和卷积神经网络</h1><h2 id=\"卷积\"><a href=\"#卷积\" class=\"headerlink\" title=\"卷积\"></a>卷积</h2><ul>\n<li><p>概念</p>\n<p>一个系统，输入不稳定，输出稳定，用卷积求系统存量</p>\n<p>图像卷积操作：周围像素点对当前像素点如何产生影响</p>\n<p>一个过滤器的卷积核：一个像素点如何试探周围像素点，如何筛选图像特征</p>\n<p><strong>但深度学习中的卷积实际上是指互相关</strong></p>\n</li>\n</ul>\n<h2 id=\"卷积神经网络概述\"><a href=\"#卷积神经网络概述\" class=\"headerlink\" title=\"卷积神经网络概述\"></a>卷积神经网络概述</h2><ul>\n<li><p>卷积神经网络包括全连接层（fully connected layers），卷积层（Convolution layer），池化层（Pooling layer）</p>\n</li>\n<li><p>将图像拆分成对应特征，称为卷积核，扫描查看目标图有无对应卷积核来确认是否为目标物体</p>\n</li>\n<li><p>用卷积核扫描目标图得出一个二维矩阵（即特征图）</p>\n</li>\n<li><p>处理边缘时的操作叫<strong>填充</strong>（Padding），如果对图像采用最大池化，或者要保持图像大小不变，则需要在边缘补零来提取边缘特征</p>\n</li>\n<li><p>对于一个有大量细节，或者说相当数量分层级的细节来说，算法复杂度过高，所以有池化，即<strong>缩小特征图</strong>（Feature Map），池化要求<strong>保留原特征图的特征</strong></p>\n<p>最常用的两种池化：</p>\n<ul>\n<li><p>最大池化：选择被扫描区域内的最大值</p>\n</li>\n<li><p>平均池化：取被扫描区域内的平均值</p>\n</li>\n</ul>\n</li>\n<li><p>卷积计算中一个基本流程：卷积，ReLU（修正线性单元），池化（下采样）</p>\n</li>\n<li><p>最后把特征图展开得到特征数组，得到概率数，用大数据修正卷积核和全连接，然后用反向传播不断训练得到近似的权重</p>\n</li>\n<li><p>开始卷积核和权重是随机的，通过大量的数据和反馈来训练得到参数</p>\n</li>\n</ul>\n<h2 id=\"从全连接到卷积层\"><a href=\"#从全连接到卷积层\" class=\"headerlink\" title=\"从全连接到卷积层\"></a>从全连接到卷积层</h2><ul>\n<li><p>对于大图片来说使用单隐藏层MLP导致所需内存过大，不适用于对图片的分类</p>\n</li>\n<li><p>卷积核遵循两个原则：<strong>平移不变性</strong>，<strong>局部性</strong></p>\n</li>\n<li><p>输出Y是扫描整个图像后与卷积核对应大小的<strong>X</strong>和<strong>W</strong>（卷积核）的<strong>哈达玛积</strong>求和（不是矩阵乘法！！！）</p>\n</li>\n<li><p>重新考察全连接层</p>\n<ul>\n<li><p>输入和输出变形为矩阵（宽度，高度）</p>\n<ul>\n<li><p>原先是将矩阵打平，现在需要空间信息，所以要保留形状</p>\n</li>\n<li><p>因为输入输出都加了一维，所以权重变为<strong>四维张量</strong>（相当于输入为k*l大小的矩阵，输出为i*j大小的矩阵）</p>\n</li>\n</ul>\n</li>\n<li><p>卷积核不能随位置的变化而变化（平移不变性）</p>\n<ul>\n<li><p>当在图片中形成一个识别器后，在一定像素大小的范围内，它都有自己的权重，当这个识别器在图片上换位置之后，他的权重应该不变</p>\n</li>\n<li><p>对于一张图片应该有多个卷积核，但是<strong>每个卷积核要识别一个不同的特征</strong>，一个卷积核就是一个识别器</p>\n</li>\n</ul>\n</li>\n<li><p>将卷积核大小限制在一定范围内，只关注x(i,j)附近的值</p>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"卷积层\"><a href=\"#卷积层\" class=\"headerlink\" title=\"卷积层\"></a>卷积层</h2><ul>\n<li><p>二维卷积层</p>\n<p>  h：height，w：weight</p>\n<p>  输入<strong>X</strong>：nh * nw</p>\n<p>  核<strong>w</strong>：kh * kw</p>\n<p>  偏差b ∈ R</p>\n<p>  输出<strong>Y</strong>：（nh - kh + 1）*（nw - kw + 1） </p>\n</li>\n<li><p>交叉相关 VS 卷积</p>\n<p>  卷积核（实际上是二位交叉相关）旋转180°得到二维卷积</p>\n<p>  由于对称性，在实际使用中没有区别</p>\n</li>\n<li><p>一维和三维交叉相关</p>\n<p>  一维：文本，语言，时序序列</p>\n<p>  三维：视频，医学图像，气象地图</p>\n<p>  二维图象是主流</p>\n</li>\n</ul>\n<p><strong>卷积解决了权重随输入变得过大的问题</strong></p>\n<ul>\n<li>代码实现</li>\n</ul>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 二维互相关运算</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">corr2d</span>(<span class=\"params\">X, K</span>):</span><br><span class=\"line\">    h, w = K.shape</span><br><span class=\"line\">    Y = torch.zeros((X.shape[<span class=\"number\">0</span>] - h + <span class=\"number\">1</span>, X.shape[<span class=\"number\">1</span>] - w + <span class=\"number\">1</span>))</span><br><span class=\"line\">    <span class=\"comment\"># 输出的大小为（X.shape[0] - K.shape[0] + 1）*（X.shape[1] - K.shape[1] + 1）</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(Y.shape[<span class=\"number\">0</span>]):</span><br><span class=\"line\">        <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(Y.shape[<span class=\"number\">1</span>]):</span><br><span class=\"line\">        <span class=\"comment\"># X（输入）和K（卷积核）哈达玛积求和</span></span><br><span class=\"line\">        <span class=\"comment\"># 遍历X中与K大小相同的所有矩阵</span></span><br><span class=\"line\">            Y[i, j] = (X[i:i + h, j:j + w] * K).<span class=\"built_in\">sum</span>()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> Y</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># X = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])</span></span><br><span class=\"line\"><span class=\"comment\"># K = torch.tensor([[0.0, 1.0], [2.0, 3.0]])</span></span><br><span class=\"line\"><span class=\"comment\"># print(corr2d(X, K))</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 实现二维卷积层</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Conv2D</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, kernel_size</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        self.weight = nn.Parameter(torch.rand(kernel_size))</span><br><span class=\"line\">        <span class=\"comment\"># bias计算利用了广播机制</span></span><br><span class=\"line\">        self.bias = nn.Parameter(torch.zeros(<span class=\"number\">1</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> corr2d(x, self.weight) + self.bias</span><br><span class=\"line\"></span><br><span class=\"line\">X = torch.ones((<span class=\"number\">6</span>, <span class=\"number\">8</span>))</span><br><span class=\"line\">X[:, <span class=\"number\">2</span>:<span class=\"number\">6</span>] = <span class=\"number\">0</span></span><br><span class=\"line\">K = torch.tensor([[<span class=\"number\">1.0</span>, -<span class=\"number\">1.0</span>]])</span><br><span class=\"line\">Y = corr2d(X, K)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># nn.Conv2d的参数：通道数，批量大小数，卷积核大小等</span></span><br><span class=\"line\">conv2d = nn.Conv2d(<span class=\"number\">1</span>, <span class=\"number\">1</span>, kernel_size=(<span class=\"number\">1</span>, <span class=\"number\">2</span>), bias=<span class=\"literal\">False</span>)</span><br><span class=\"line\">X = X.reshape((<span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">6</span>, <span class=\"number\">8</span>))</span><br><span class=\"line\">Y = Y.reshape((<span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">6</span>, <span class=\"number\">7</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">10</span>):</span><br><span class=\"line\">    Y_hat = conv2d(X)</span><br><span class=\"line\">    l = (Y_hat - Y)**<span class=\"number\">2</span></span><br><span class=\"line\">    conv2d.zero_grad()</span><br><span class=\"line\">    l.<span class=\"built_in\">sum</span>().backward()</span><br><span class=\"line\">    <span class=\"comment\"># 学习率：3e-2</span></span><br><span class=\"line\">    <span class=\"comment\"># 手写一个权重衰退</span></span><br><span class=\"line\">    conv2d.weight.data[:] -= <span class=\"number\">3e-2</span> * conv2d.weight.grad</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(i + <span class=\"number\">1</span>) % <span class=\"number\">2</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">f'batch <span class=\"subst\">{i + <span class=\"number\">1</span>}</span>, loss <span class=\"subst\">{l.<span class=\"built_in\">sum</span>(): <span class=\"number\">.3</span>f}</span>'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">conv2d.weight.data.reshape((<span class=\"number\">1</span>, <span class=\"number\">2</span>))</span><br></pre></td></tr></tbody></table></figure>\n\n<h2 id=\"卷积层里的填充与步幅\"><a href=\"#卷积层里的填充与步幅\" class=\"headerlink\" title=\"卷积层里的填充与步幅\"></a>卷积层里的填充与步幅</h2><ul>\n<li><p>填充</p>\n<ul>\n<li><p>原因：更大的卷积核可以更快地减小输出大小，形状从nh * nw较少到（nh - kh + 1）*（nw - kw + 1）</p>\n</li>\n<li><p>填充（paddinng）：在输入周围添加额外的行/列，来<strong>控制输出形状的减少量</strong></p>\n</li>\n<li><p>填充ph行和pw列，输出形状为（nh - kh + ph + 1）*（nh - kh + ph + 1）</p>\n</li>\n<li><p><strong>通常取ph = kh - 1，pw = kw - 1</strong>（因为可以消去nh - kh + ph + 1中的- kh + 1）</p>\n</li>\n<li><p>当kh为奇数：在上下两侧填充ph/2</p>\n</li>\n<li><p>当kh为偶数：在上侧填充⌈ph/2⌉（向上取整），在下侧填充⌊ph/2⌋（向下取整）</p>\n</li>\n</ul>\n</li>\n<li><p>步幅</p>\n<ul>\n<li><p>步幅（stride）是指卷积核在行/列的滑动步长，可以<strong>成倍地减少输出形状</strong></p>\n</li>\n<li><p>给定高度sh和宽度sw的步幅，输出形状是⌊(nh − kh + ph + sh )/sh ⌋ × ⌊(nw − kw + pw + sw )/sw ⌋（向下取整）</p>\n</li>\n<li><p>如果ph = kh - 1，pw = kw - 1，输出形状为⌊(nh + sh − 1)/sh ⌋ × ⌊(nw + sw − 1)/sw ⌋</p>\n</li>\n<li><p>如果输入高度和宽度可以被步幅整除  (nh /sh) × (nw /sw)</p>\n</li>\n</ul>\n</li>\n<li><p>代码实现</p>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">comp_conv2d</span>(<span class=\"params\">conv2d, X</span>):</span><br><span class=\"line\">    X = X.reshape((<span class=\"number\">1</span>, <span class=\"number\">1</span>) + X.shape)</span><br><span class=\"line\">    Y = conv2d(X)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> Y.reshape(Y.shape[<span class=\"number\">2</span>:])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 上下左右各填充一行</span></span><br><span class=\"line\">conv2d = nn.Conv2d(<span class=\"number\">1</span>, <span class=\"number\">1</span>, kernel_size=<span class=\"number\">3</span>, padding=<span class=\"number\">1</span>)</span><br><span class=\"line\">X = torch.rand(size=(<span class=\"number\">8</span>, <span class=\"number\">8</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(comp_conv2d(conv2d, X).shape)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 将高度和宽度步幅设为2</span></span><br><span class=\"line\">conv2d = nn.Conv2d(<span class=\"number\">1</span>, <span class=\"number\">1</span>, kernel_size=<span class=\"number\">3</span>, padding=<span class=\"number\">1</span>, stride=<span class=\"number\">2</span>)</span><br><span class=\"line\">comp_conv2d(conv2d, X).shape</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 稍微复杂一点的例子</span></span><br><span class=\"line\">conv2d = nn.Conv2d(<span class=\"number\">1</span>, <span class=\"number\">1</span>, kernel_size=(<span class=\"number\">3</span>, <span class=\"number\">5</span>), padding=(<span class=\"number\">0</span>, <span class=\"number\">1</span>), stride=(<span class=\"number\">3</span>, <span class=\"number\">4</span>))</span><br><span class=\"line\">comp_conv2d(conv2d, X).shape</span><br><span class=\"line\"><span class=\"comment\"># 可以用过控制填充和步幅来控制图片长宽比</span></span><br></pre></td></tr></tbody></table></figure>\n\n<h2 id=\"卷积层里的多输入多输出通道\"><a href=\"#卷积层里的多输入多输出通道\" class=\"headerlink\" title=\"卷积层里的多输入多输出通道\"></a>卷积层里的多输入多输出通道</h2><ul>\n<li><p>多个输入通道<br>  彩色图像可能有RGB三个通道<br>  转换为灰度会丢失信息<br>  每个通道都有一个卷积核，结果是所有卷积结果的和</p>\n<p>  输入<strong>X</strong>：ci × nh × nw<br>  核<strong>W</strong>：ci × kh × kw<br>  输入<strong>Y</strong>：mh × mw</p>\n</li>\n<li><p>多个输出通道<br>  可以有多个三维卷积核，每个核生成一个输出通道，即<strong>每个核对应图片的一个特征</strong></p>\n<p>  输入<strong>X</strong>：ci × nh × nw<br>  核<strong>W</strong>：c0 × ci × kh ×kw<br>  输出<strong>Y</strong>：c0 × mh × mw</p>\n</li>\n<li><p>多个输入和输出通道<br>  每个输出通道可以识别特定模式<br>  输入通道核识别并组合输入中的模式</p>\n</li>\n<li><p>1 × 1卷积核<br>  kh = kw = 1是一个受欢迎的选择。它不识别空间模式，<strong>只是融合通道</strong><br>  因为他不能识别当前像素点周围的信息，即不能识别空间信息<br>  <strong>相当于输入形状为nhnw × ci，权重为c0 × ci的全连接层</strong></p>\n</li>\n<li><p>二维卷积层<br>  输入<strong>X</strong>：ci × nh × nw<br>  核<strong>W</strong>：c0 × ci × kh × kw<br>  偏差<strong>B</strong>：co × ci<br>  输出<strong>Y</strong>：c0 × mh × mw</p>\n</li>\n<li><p>代码实现</p>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">corr2d_multi_in</span>(<span class=\"params\">X, K</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"built_in\">sum</span>(d2l.corr2d(x, k) <span class=\"keyword\">for</span> x, k <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>(X, K))</span><br><span class=\"line\"></span><br><span class=\"line\">X = torch.tensor([[[<span class=\"number\">0.0</span>, <span class=\"number\">1.0</span>, <span class=\"number\">2.0</span>], [<span class=\"number\">3.0</span>, <span class=\"number\">4.0</span>, <span class=\"number\">5.0</span>], [<span class=\"number\">6.0</span>, <span class=\"number\">7.0</span>, <span class=\"number\">8.0</span>]],</span><br><span class=\"line\">                  [[<span class=\"number\">1.0</span>, <span class=\"number\">2.0</span>, <span class=\"number\">3.0</span>], [<span class=\"number\">4.0</span>, <span class=\"number\">5.0</span>, <span class=\"number\">6.0</span>], [<span class=\"number\">7.0</span>, <span class=\"number\">8.0</span>, <span class=\"number\">9.0</span>]]])</span><br><span class=\"line\">K = torch.tensor([[[<span class=\"number\">0.0</span>, <span class=\"number\">1.0</span>], [<span class=\"number\">2.0</span>, <span class=\"number\">3.0</span>]], [[<span class=\"number\">1.0</span>, <span class=\"number\">2.0</span>], [<span class=\"number\">3.0</span>, <span class=\"number\">4.0</span>]]])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(corr2d_multi_in(X, K))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">corr2d_multi_in_out</span>(<span class=\"params\">X, K</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> torch.stack([corr2d_multi_in(X, k) <span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> K], <span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># concat沿着已有的维度拼接，stack在新创建的维度上拼接</span></span><br><span class=\"line\">K = torch.stack((K, K + <span class=\"number\">1</span>, K + <span class=\"number\">2</span>), <span class=\"number\">0</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(K.shape)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(corr2d_multi_in_out(X, K))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 1 x 1 卷积</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">corr2d_multi_in_out_1x1</span>(<span class=\"params\">X, K</span>):</span><br><span class=\"line\">    c_i, h, w = X.shape[<span class=\"number\">0</span>]</span><br><span class=\"line\">    c_o = K.shape[<span class=\"number\">0</span>]</span><br><span class=\"line\">    X = X.reshape((c_i, h * w))</span><br><span class=\"line\">    K = K.reshape((c_o, c_i))</span><br><span class=\"line\">    Y = torch.matmul(K, X)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> Y.reshape(c_o, h, w)</span><br><span class=\"line\"></span><br><span class=\"line\">X = torch.normal(<span class=\"number\">0</span>, <span class=\"number\">1</span>, (<span class=\"number\">3</span>, <span class=\"number\">3</span>, <span class=\"number\">3</span>))</span><br><span class=\"line\"><span class=\"comment\"># K的输出通道为2，即有两个卷积核，输入通道为3</span></span><br><span class=\"line\">K = torch.normal(<span class=\"number\">0</span>, <span class=\"number\">1</span>, (<span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">1</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">Y1 = corr2d_multi_in_out_1x1(X, K)</span><br><span class=\"line\">Y2 = corr2d_multi_in_out(X, K)</span><br><span class=\"line\"><span class=\"keyword\">assert</span> <span class=\"built_in\">float</span>(torch.<span class=\"built_in\">abs</span>(Y1 - Y2).<span class=\"built_in\">sum</span>()) &lt; <span class=\"number\">1e-6</span></span><br></pre></td></tr></tbody></table></figure>\n\n",
            "tags": [
                "深度学习"
            ]
        },
        {
            "id": "https://chencc8.github.io/2022/03/11/shen-du-xue-xi-pytorch-shen-jing-wang-luo-ji-chu/",
            "url": "https://chencc8.github.io/2022/03/11/shen-du-xue-xi-pytorch-shen-jing-wang-luo-ji-chu/",
            "title": "深度学习-PyTorch神经网络基础",
            "date_published": "2022-03-11T10:55:30.000Z",
            "content_html": "<h1 id=\"深度学习-PyTorch神经网络基础\"><a href=\"#深度学习-PyTorch神经网络基础\" class=\"headerlink\" title=\"深度学习-PyTorch神经网络基础\"></a>深度学习-PyTorch神经网络基础</h1><h2 id=\"模型构造\"><a href=\"#模型构造\" class=\"headerlink\" title=\"模型构造\"></a>模型构造</h2><figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.nn <span class=\"keyword\">import</span> functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"><span class=\"comment\"># 定义了一些没有包括参数的函数</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Module是torch核心</span></span><br><span class=\"line\">net = nn.Sequential(nn.Linear(<span class=\"number\">20</span>, <span class=\"number\">256</span>), nn.ReLU(), nn.Linear(<span class=\"number\">256</span>, <span class=\"number\">10</span>))</span><br><span class=\"line\"><span class=\"comment\"># 这里参数自动初始化</span></span><br><span class=\"line\"><span class=\"comment\"># nn.Sequential定义了一个特殊的Module</span></span><br><span class=\"line\"><span class=\"comment\"># 任何一个层或神经网络都是Module的子类</span></span><br><span class=\"line\"></span><br><span class=\"line\">X = torch.rand(<span class=\"number\">2</span>, <span class=\"number\">20</span>)</span><br><span class=\"line\"><span class=\"comment\"># print(net(X))</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 多层感知机的实现</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">MLP</span>(nn.Module):</span><br><span class=\"line\"><span class=\"comment\"># 是nn.Module的子类</span></span><br><span class=\"line\"><span class=\"comment\"># 所有的Module都有这两种重要的方法</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        self.hidden = nn.Linear(<span class=\"number\">20</span>, <span class=\"number\">256</span>)</span><br><span class=\"line\">        self.out = nn.Linear(<span class=\"number\">256</span>, <span class=\"number\">10</span>)</span><br><span class=\"line\">        </span><br><span class=\"line\"><span class=\"comment\"># 前向函数</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, X</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.out(F.relu(self.hidden(X)))</span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"comment\"># net = MLP()</span></span><br><span class=\"line\"><span class=\"comment\"># print(net(X))</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># forward函数不需要调用是因为nn.Module中将__call__函数等价于forward函数。</span></span><br><span class=\"line\"><span class=\"comment\"># Sequential类的作用：把传进来的层按顺序存添加到模块中</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">MySequential</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, *args</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        <span class=\"keyword\">for</span> block <span class=\"keyword\">in</span> args:</span><br><span class=\"line\">            self._modules[block] = block</span><br><span class=\"line\"><span class=\"comment\"># 对所有传进来的层都放在_modules特殊的容器中</span></span><br><span class=\"line\"><span class=\"comment\"># _module是一个ordered dict</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, X</span>):</span><br><span class=\"line\">        <span class=\"keyword\">for</span> block <span class=\"keyword\">in</span> self._modules.values():</span><br><span class=\"line\">            X = block(X)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> X</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))</span></span><br><span class=\"line\"><span class=\"comment\"># print(net(X))</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">FixedHiddenMLP</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        <span class=\"comment\"># rand_weight不参加训练，不能计算梯度</span></span><br><span class=\"line\">        self.rand_weight = torch.rand((<span class=\"number\">20</span>, <span class=\"number\">20</span>), requires_grad=<span class=\"literal\">False</span>)</span><br><span class=\"line\">        <span class=\"comment\"># 非首次初始化需要设置require_grad为True</span></span><br><span class=\"line\">        self.linear = nn.Linear(<span class=\"number\">20</span>, <span class=\"number\">20</span>)</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, X</span>):</span><br><span class=\"line\">        X = self.linear(X)</span><br><span class=\"line\">        X = F.relu(torch.mm(X, self.rand_weight) + <span class=\"number\">1</span>)</span><br><span class=\"line\">        X = self.linear(X)</span><br><span class=\"line\">        <span class=\"keyword\">while</span> X.<span class=\"built_in\">abs</span>().<span class=\"built_in\">sum</span>() &gt; <span class=\"number\">1</span>:</span><br><span class=\"line\">            X /= <span class=\"number\">2</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> X.<span class=\"built_in\">sum</span>()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># net = FixedHiddenMLP()</span></span><br><span class=\"line\"><span class=\"comment\"># print(net(X))</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 可以嵌套使用</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">NestMLP</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        self.net = nn.Sequential(nn.Linear(<span class=\"number\">20</span>, <span class=\"number\">64</span>), nn.ReLU(),</span><br><span class=\"line\">                                 nn.Linear(<span class=\"number\">64</span>, <span class=\"number\">32</span>), nn.ReLU())</span><br><span class=\"line\">        self.linear = nn.Linear(<span class=\"number\">32</span>, <span class=\"number\">16</span>)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, X</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.linear(self.net(X))</span><br><span class=\"line\"></span><br><span class=\"line\">chimera = nn.Sequential(NestMLP(), nn.Linear(<span class=\"number\">16</span>, <span class=\"number\">20</span>), FixedHiddenMLP())</span><br><span class=\"line\">chimera(X)</span><br></pre></td></tr></tbody></table></figure>\n\n<h2 id=\"参数管理\"><a href=\"#参数管理\" class=\"headerlink\" title=\"参数管理\"></a>参数管理</h2><figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">net = nn.Sequential(nn.Linear(<span class=\"number\">4</span>, <span class=\"number\">8</span>), nn.ReLU(), nn.Linear(<span class=\"number\">8</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\">X = torch.rand(size=(<span class=\"number\">2</span>, <span class=\"number\">4</span>))</span><br><span class=\"line\">net(X)</span><br><span class=\"line\"><span class=\"comment\"># state 状态 OrderedDict类</span></span><br><span class=\"line\"><span class=\"comment\"># net[2]表示最后一层</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(net[<span class=\"number\">2</span>].state_dict())</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">type</span>(net[<span class=\"number\">2</span>].bias))</span><br><span class=\"line\"><span class=\"comment\"># &lt;class 'torch.nn.parameter.Parameter'&gt;</span></span><br><span class=\"line\"><span class=\"comment\"># parameter定义的是可以优化的参数</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(net[<span class=\"number\">2</span>].bias)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(net[<span class=\"number\">2</span>].bias.data)</span><br><span class=\"line\"><span class=\"comment\"># 通过.data和.grad访问参数值和梯度</span></span><br><span class=\"line\"><span class=\"comment\"># 没有进行反向传播，所以没有梯度</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(net[<span class=\"number\">2</span>].weight.grad == <span class=\"literal\">None</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 一次性访问所有参数</span></span><br><span class=\"line\"><span class=\"comment\"># *号解压可迭代对象,在列表前加*号,会将列表拆分成一个一个的独立元素,不光是列表、元组、字典，</span></span><br><span class=\"line\"><span class=\"comment\"># 通过named_parameters()函数获取整个网络的参数</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(*[(name, param.shape) <span class=\"keyword\">for</span> name, param <span class=\"keyword\">in</span> net[<span class=\"number\">0</span>].named_parameters()])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(*[(name, param.shape) <span class=\"keyword\">for</span> name, param <span class=\"keyword\">in</span> net.named_parameters()])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(net.state_dict()[<span class=\"string\">'2.bias'</span>].data)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 从嵌套块收集参数</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">block1</span>():</span><br><span class=\"line\">    <span class=\"keyword\">return</span> nn.Sequential(nn.Linear(<span class=\"number\">4</span>, <span class=\"number\">8</span>), nn.ReLU(), nn.Linear(<span class=\"number\">8</span>, <span class=\"number\">4</span>),</span><br><span class=\"line\">                         nn.ReLU())</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">block2</span>():</span><br><span class=\"line\">    net = nn.Sequential()</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">4</span>):</span><br><span class=\"line\">        net.add_module(<span class=\"string\">f'block<span class=\"subst\">{i}</span>'</span>, block1())</span><br><span class=\"line\">    <span class=\"keyword\">return</span> net</span><br><span class=\"line\"></span><br><span class=\"line\">rgnet = nn.Sequential(block2(), nn.Linear(<span class=\"number\">4</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(rgnet(X))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(rgnet)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 内置初始化</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">init_normal</span>(<span class=\"params\">m</span>):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">type</span>(m) == nn.Linear:</span><br><span class=\"line\">        <span class=\"comment\"># 判断是否为全连接层</span></span><br><span class=\"line\">        <span class=\"comment\"># 加_表示替换函数，直接改变输入的值，不返回值</span></span><br><span class=\"line\">        nn.init.normal_(m.weight, mean=<span class=\"number\">0</span>, std=<span class=\"number\">0.01</span>)</span><br><span class=\"line\">        nn.init.zeros_(m.bias)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># apply()对所有的层进行遍历修改参数</span></span><br><span class=\"line\"><span class=\"comment\"># 给一个方式遍历整个神经网络进行修改</span></span><br><span class=\"line\">net.apply(init_normal)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(net[<span class=\"number\">0</span>].weight.data[<span class=\"number\">0</span>], net[<span class=\"number\">0</span>].bias.data[<span class=\"number\">0</span>])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">init_constant</span>(<span class=\"params\">m</span>):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">type</span>(m) == nn.Linear:</span><br><span class=\"line\">       nn.init.constant_(m.weight, <span class=\"number\">1</span>)</span><br><span class=\"line\">       nn.init.zeros_(m.bias)</span><br><span class=\"line\"></span><br><span class=\"line\">net.apply(init_constant)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(net[<span class=\"number\">0</span>].weight.data[<span class=\"number\">0</span>], net[<span class=\"number\">0</span>].bias.data[<span class=\"number\">0</span>])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 对某些块使用不同的初始化</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">xavier</span>(<span class=\"params\">m</span>):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">type</span>(m) == nn.Linear:</span><br><span class=\"line\">        nn.init.xavier_uniform_(m.weight)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">init_42</span>(<span class=\"params\">m</span>):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">type</span>(m) == nn.Linear:</span><br><span class=\"line\">        nn.init.constant_(m.weight, <span class=\"number\">42</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 自定义初始化</span></span><br><span class=\"line\"><span class=\"comment\"># 可以直接替换修改或者自定义一个初始化方法</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 参数绑定</span></span><br><span class=\"line\"><span class=\"comment\"># 参数共享</span></span><br><span class=\"line\">shared = nn.Linear(<span class=\"number\">8</span>, <span class=\"number\">8</span>)</span><br><span class=\"line\"><span class=\"comment\"># 若修改其中一个参数，共享的参数也改变</span></span><br><span class=\"line\">net = nn.Sequential(nn.Linear(<span class=\"number\">4</span>, <span class=\"number\">8</span>), nn.ReLU(), shared, nn.ReLU(), shared, nn.ReLU(), nn.Linear(<span class=\"number\">8</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\">net(X)</span><br></pre></td></tr></tbody></table></figure>\n\n<h2 id=\"自定义层\"><a href=\"#自定义层\" class=\"headerlink\" title=\"自定义层\"></a>自定义层</h2><figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">CenteredLayer</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, X</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> X - X.mean()</span><br><span class=\"line\"></span><br><span class=\"line\">layer = CenteredLayer()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(layer(torch.FloatTensor([<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>])))</span><br><span class=\"line\"></span><br><span class=\"line\">net = nn.Sequential(nn.Linear(<span class=\"number\">8</span>, <span class=\"number\">128</span>), CenteredLayer())</span><br><span class=\"line\">Y = net(torch.rand(<span class=\"number\">4</span>, <span class=\"number\">8</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(Y.mean())</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 带参数的层</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">MyLinear</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, in_units, units</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        <span class=\"comment\"># 需要把参数放进Parameter里</span></span><br><span class=\"line\">        self.weight = nn.Parameter(torch.randn(in_units, units))</span><br><span class=\"line\">        self.bias = nn.Parameter(torch.randn(units, ))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, X</span>):</span><br><span class=\"line\">        linear = torch.matmul(X, self.weight.data) + self.bias.data</span><br><span class=\"line\">        <span class=\"keyword\">return</span> F.relu(linear)</span><br><span class=\"line\"></span><br><span class=\"line\">dense = MyLinear(<span class=\"number\">5</span>, <span class=\"number\">3</span>)</span><br><span class=\"line\">dense.weight</span><br></pre></td></tr></tbody></table></figure>\n\n<h2 id=\"读取文件\"><a href=\"#读取文件\" class=\"headerlink\" title=\"读取文件\"></a>读取文件</h2><figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 读写文件</span></span><br><span class=\"line\">x = torch.arange(<span class=\"number\">4</span>)</span><br><span class=\"line\">torch.save(x, <span class=\"string\">'x-file'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">x2 = torch.load(<span class=\"string\">'x-file'</span>)</span><br><span class=\"line\">x2</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 存储列表</span></span><br><span class=\"line\">y = torch.zeros(<span class=\"number\">4</span>)</span><br><span class=\"line\">torch.save([x, y], <span class=\"string\">'x-files'</span>)</span><br><span class=\"line\">x2, y2 = torch.load(<span class=\"string\">'x-files'</span>)</span><br><span class=\"line\">(x2, y2)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 存储字典</span></span><br><span class=\"line\">mydict = {<span class=\"string\">'x'</span>: x, <span class=\"string\">'y'</span>: y}</span><br><span class=\"line\">torch.save(mydict, <span class=\"string\">'mydict'</span>)</span><br><span class=\"line\">mydict2 = torch.load(<span class=\"string\">'mydict'</span>)</span><br><span class=\"line\">mydict2</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 加载和保存模型参数</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">MLP</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        self.hidden = nn.Linear(<span class=\"number\">20</span>, <span class=\"number\">256</span>)</span><br><span class=\"line\">        self.output = nn.Linear(<span class=\"number\">256</span>, <span class=\"number\">10</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.output(F.relu(self.hidden(x)))</span><br><span class=\"line\"></span><br><span class=\"line\">net = MLP()</span><br><span class=\"line\">X = torch.randn(size=(<span class=\"number\">2</span>, <span class=\"number\">20</span>))</span><br><span class=\"line\">Y = net(X)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># pytorch不能把整个模型的定义存储下来（有别的方法）</span></span><br><span class=\"line\"><span class=\"comment\"># 存储权重即可，但读取的时候要重新实例化一个模型，计算部分不存储</span></span><br><span class=\"line\"><span class=\"comment\"># 读取时需要把模型定义带走</span></span><br><span class=\"line\"><span class=\"comment\"># 将模型参数存储为字典</span></span><br><span class=\"line\">torch.save(net.state_dict(), <span class=\"string\">'mlp.params'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># load</span></span><br><span class=\"line\">clone = MLP()</span><br><span class=\"line\">clone.load_state_dict(torch.load(<span class=\"string\">'mlp.params'</span>))</span><br><span class=\"line\">clone.<span class=\"built_in\">eval</span>()</span><br><span class=\"line\"></span><br><span class=\"line\">Y_clone = clone(X)</span><br><span class=\"line\">Y_clone == Y</span><br></pre></td></tr></tbody></table></figure>",
            "tags": [
                "深度学习"
            ]
        },
        {
            "id": "https://chencc8.github.io/2022/03/08/shen-du-xue-xi-shu-zhi-wen-ding-xing-mo-xing-chu-shi-hua-he-ji-huo-han-shu/",
            "url": "https://chencc8.github.io/2022/03/08/shen-du-xue-xi-shu-zhi-wen-ding-xing-mo-xing-chu-shi-hua-he-ji-huo-han-shu/",
            "title": "深度学习-数值稳定性+模型初始化和激活函数",
            "date_published": "2022-03-08T06:43:30.000Z",
            "content_html": "<h1 id=\"深度学习-数值稳定性-模型初始化和激活函数\"><a href=\"#深度学习-数值稳定性-模型初始化和激活函数\" class=\"headerlink\" title=\"深度学习-数值稳定性+模型初始化和激活函数\"></a>深度学习-数值稳定性+模型初始化和激活函数</h1><h2 id=\"数值稳定性\"><a href=\"#数值稳定性\" class=\"headerlink\" title=\"数值稳定性\"></a>数值稳定性</h2><ul>\n<li><p>概述</p>\n<p>  有关数值稳定性的典型问题是衰减和爆炸</p>\n</li>\n<li><p>来源</p>\n<p>  设神经网络有d层，计算损失l关于参数wt的梯度，需要做（d-t）次矩阵乘法，容易引起梯度爆炸和梯度消失，<strong>本质上都是由于深度神经网络的反向传播造成的</strong></p>\n</li>\n<li><p>梯度爆炸的问题</p>\n<ul>\n<li><p>值超出值域（infinity）</p>\n<p>  对于16位浮点数尤为严重（数值区间6e-5 - 6e4）</p>\n</li>\n<li><p>对学习率敏感<br>  学习率太大 -&gt; 大参数值 -&gt; 更大的梯度</p>\n<p>  （反向传播，学习率变大 -&gt; 参数值变大 -&gt; 梯度变大）</p>\n<p>  学习率太小 -&gt; 训练无进展</p>\n<p>  需要在训练过程中不管调整学习率</p>\n</li>\n</ul>\n</li>\n<li><p>梯度消失的问题</p>\n<p>  以sigmoid激活函数为例，当输入较大时，sigmoid激活函数的梯度变得很小，趋近于0。</p>\n<ul>\n<li><p>梯度值变成0</p>\n<p>  对于16位浮点数尤为严重（数值区间6e-5 - 6e4）</p>\n</li>\n<li><p>训练没有进展（不管如何选择学习率）</p>\n</li>\n<li><p>对于底部层尤为严重</p>\n<p>  仅仅顶部层训练的较好</p>\n<p>  无法让神经网络更深（无论有多少层，都和一个浅层神经网络类似）</p>\n</li>\n</ul>\n</li>\n<li><p>关于学习率η</p>\n<ul>\n<li><p>学习率过大，会导致<strong>参数弹跳</strong>，损失项<strong>可能不会在每次迭代都下降</strong>，使损失项错过最小值，反而导致参数变大，使梯度变大。</p>\n</li>\n<li><p>学习率过小，难以到达最小值，收敛的很慢，导致训练无进展</p>\n</li>\n<li><p>推荐学习率：0.001， 0.003，0.01， 0.03， 0.1， 0.3， 1 ……（成三倍增长）</p>\n</li>\n</ul>\n<p>  （自动收敛测试方法：当损失项的值小于一个很小的值ε（可以是1e-3）时，判断为已收敛，但是一般来说，选取一个合适的阈值ε是非常困难的，所以要找到一个合适的阈值更好的方法还是画曲线图。）</p>\n</li>\n</ul>\n<h2 id=\"模型初始化\"><a href=\"#模型初始化\" class=\"headerlink\" title=\"模型初始化\"></a>模型初始化</h2><ul>\n<li><p>让训练更加稳定</p>\n<p>  目标：让梯度值在合理的范围内（如[1e-6, 1e3]）</p>\n<p>  方法：</p>\n<ul>\n<li><p>将乘法变加法，如ResNet，LSTM</p>\n</li>\n<li><p>归一化，如梯度归一化（如把梯度改为均值，方差为固定数值的数，把梯度拉到一定范围内），梯度裁剪（强行把梯度减到一个范围内，如大于5则变为5，小于-5则变为-5）</p>\n</li>\n<li><p>合理的权重初始和激活函数</p>\n</li>\n</ul>\n</li>\n<li><p>让每层的方差是一个常数</p>\n<ul>\n<li><p>将每层的输出和梯度都看作随机变量</p>\n</li>\n<li><p>让他们的均值和方差都保持一致</p>\n</li>\n</ul>\n</li>\n<li><p>权重初始化</p>\n<ul>\n<li><p>在合理值区间里随机出是参数</p>\n</li>\n<li><p>训练开始的时候更容易有数值不稳定</p>\n<ol>\n<li><p>远离最优解的地方损失函数表面可能很复杂。（导致梯度很大，从而导致更大的参数值）</p>\n</li>\n<li><p>最优解附近表面可能会比较平</p>\n</li>\n</ol>\n</li>\n<li><p>使用N(0, 0.01)来初始可能对小网络没问题，但不能保证深度神经网络</p>\n</li>\n</ul>\n</li>\n<li><p>Xavier初始</p>\n<p>  原因：难以满足输入值个数(nt-1)和输出值个数(nt)相同</p>\n<p>  Xavier使得  t层方差   = （t层输入值个数）+（t层输出值个数）/  2</p>\n<p>  适配权重形状变换，特别是nt</p>\n</li>\n</ul>\n<h2 id=\"激活函数\"><a href=\"#激活函数\" class=\"headerlink\" title=\"激活函数\"></a>激活函数</h2><ul>\n<li><p>据推理可得所使用的激活函数必须是它本身，即σ(x) = x</p>\n<p>  sigmoid(x) = 1/2 + x/4 - x^3 / 48 + O(x^5)</p>\n<p>  relu(x) = 0 + x     for x &gt;= 0</p>\n<p>  tanh(x) = 0 + x - x^3 / 3 + O(x*x*x*x*x)</p>\n<p>  relu(x)与tanh(x)符合条件</p>\n<p>  sigmoid函数需要调整为  4*sigmoid(x)-2</p>\n</li>\n</ul>\n<h2 id=\"QA\"><a href=\"#QA\" class=\"headerlink\" title=\"QA\"></a>QA</h2><ol>\n<li><p>Q：nan,inf怎么产生，如何让解决？</p>\n<p>A：inf通常是权重初始值太大或者学习率太大</p>\n<p>nan通常是一个值除0</p>\n<p>解决：合理初始化权重，激活函数选择，学习率不要太大</p>\n<p>把学习率一直向小调整直到inf或nan不出现</p>\n</li>\n<li><p>Q：为什么16位浮点影响严重？</p>\n<p>A：传统高性能运算是64位，python的默认数据类型是64位，32位更常用。如芯片如果用16位浮点数，比32位快2倍，比64位快4倍。对做训练时的梯度有影响。bf16开始取代 fp16。</p>\n</li>\n</ol>\n",
            "tags": [
                "深度学习"
            ]
        },
        {
            "id": "https://chencc8.github.io/2022/03/07/shen-du-xue-xi-diu-qi-fa/",
            "url": "https://chencc8.github.io/2022/03/07/shen-du-xue-xi-diu-qi-fa/",
            "title": "深度学习-丢弃法",
            "date_published": "2022-03-07T11:18:50.000Z",
            "content_html": "<h1 id=\"深度学习-丢弃法\"><a href=\"#深度学习-丢弃法\" class=\"headerlink\" title=\"深度学习-丢弃法\"></a>深度学习-丢弃法</h1><ul>\n<li><p>概述<br>  丢弃法也可以用来应对过拟合问题（倒置丢弃法）</p>\n</li>\n<li><p>方法</p>\n<ul>\n<li><strong>一个好的模型需要对输入数据的扰动鲁棒</strong></li>\n<li>当对隐藏层使用丢弃法时，该层的隐藏单元有一定概率被丢弃掉。设丢弃概率为p，那么p的概率为hi会被清零，有1-p的概率hi会除以1-p做拉伸</li>\n</ul>\n</li>\n<li><p>解释</p>\n<ul>\n<li>由于在训练中隐藏层神经元的丢弃是随机的，即h1,……,hi都有可能被清零，<strong>输出层的计算无法过度依赖h1,……,hi中的任一个</strong>，从而在训练模型时起到正则化的作用，并可以用来应对过拟合。</li>\n<li>在测试模型时，我们为了得到更加确定性的结果，一般<strong>不使用</strong>丢弃法。</li>\n</ul>\n</li>\n<li><p>补充</p>\n<ul>\n<li>丢弃法不改变其输入的期望值</li>\n<li>丢弃法在<strong>层中</strong>加入噪音</li>\n<li>正则项只在<strong>训练中</strong>使用</li>\n<li>使用有噪音的数据等价于Tikhonov正则</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"丢弃法从零开始实现\"><a href=\"#丢弃法从零开始实现\" class=\"headerlink\" title=\"丢弃法从零开始实现\"></a>丢弃法从零开始实现</h2><figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> d2lzh <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"><span class=\"keyword\">from</span> mxnet <span class=\"keyword\">import</span> gluon, init, nd, autograd</span><br><span class=\"line\"><span class=\"keyword\">from</span> mxnet.gluon <span class=\"keyword\">import</span> loss <span class=\"keyword\">as</span> gloss, nn</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">dropout</span>(<span class=\"params\">X, drop_prob</span>):</span><br><span class=\"line\">\t<span class=\"keyword\">assert</span> <span class=\"number\">0</span> &lt;= drop_prob &lt;= <span class=\"number\">1</span></span><br><span class=\"line\">\t<span class=\"comment\"># assert 表达式，又称断言语句，可以看作是功能缩小版的if语句，用于判断表达式的值，如果为错，python解释器会报错。</span></span><br><span class=\"line\">\tkeep_prob = <span class=\"number\">1</span> - drop_prob</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> keep_prob == <span class=\"number\">0</span>：</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> \tX.zeros_like()</span><br><span class=\"line\">\t<span class=\"comment\"># 保留概率为0，即丢弃概率为1，返回与X形状相同的全为0的矩阵</span></span><br><span class=\"line\">\tmask = nd.random.uniform(<span class=\"number\">0</span>, <span class=\"number\">1</span>, X.shape) &lt; keep_prob</span><br><span class=\"line\">\t<span class=\"comment\"># mask是一个由0和1组成的形状为X的矩阵</span></span><br><span class=\"line\">\t<span class=\"comment\"># uniform()方法将随机生成下一个实数，它在[x, y]范围内。</span></span><br><span class=\"line\">\t<span class=\"keyword\">return</span> mask * X / keep_prob</span><br><span class=\"line\">\t<span class=\"comment\"># 做乘法远比从中选元素快得多</span></span><br><span class=\"line\">\t</span><br><span class=\"line\">\tnum_inputs, num_outputs, num_hiddens1, num_hiddens2 = <span class=\"number\">784</span>, <span class=\"number\">10</span>, <span class=\"number\">256</span>, <span class=\"number\">256</span></span><br><span class=\"line\">\t<span class=\"comment\"># 两个隐藏层</span></span><br><span class=\"line\">\tw1 = nd.random.normal(scale=<span class=\"number\">0.01</span>, shape=(num_inputs, num_hiddens1))</span><br><span class=\"line\">\tb1 = nd.zeros(num_hiddens1)</span><br><span class=\"line\">\tw2 = nd.random.mormal(scale=<span class=\"number\">0.01</span>, shape=(num_hiddens1, num_hiddens2))</span><br><span class=\"line\">\tb2 = nd.zeros(num_hiddens2)</span><br><span class=\"line\">\tw3 = nd.random.normal(scale=<span class=\"number\">0.01</span>, shape=(num_hiddens2, num_outputs))</span><br><span class=\"line\">\tb3 = nd.zeros(num_outputs)</span><br><span class=\"line\">\t</span><br><span class=\"line\">\tparams = [w1, b1, w2, b2, w3, b3]</span><br><span class=\"line\">\t<span class=\"keyword\">for</span> param <span class=\"keyword\">in</span> params:</span><br><span class=\"line\">\t\tparam.attach_grad()</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"comment\"># 丢弃概率</span></span><br><span class=\"line\">\tdrop_prob1, drop_prob = <span class=\"number\">0.2</span>, <span class=\"number\">0.5</span></span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"comment\"># 相当于神经网络</span></span><br><span class=\"line\">\t<span class=\"keyword\">def</span> <span class=\"title function_\">net</span>(<span class=\"params\">X</span>):</span><br><span class=\"line\">\t\t<span class=\"comment\"># 如果shape参数中包含特殊值-1，即将该数组打平，变成一维，先行后列。</span></span><br><span class=\"line\">\t\tX = X.reshape((-<span class=\"number\">1</span>, num_inputs))</span><br><span class=\"line\">\t\tH1 = (nd.dot(X, w1) + b1).relu()</span><br><span class=\"line\">\t\t<span class=\"comment\"># 只在训练模型时使用丢弃法</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> autograd.is_training():</span><br><span class=\"line\">\t\t\tH1 = dropout(H1, drop_prob1)</span><br><span class=\"line\">\t\t\t<span class=\"comment\"># 在第一层全连接层后添加丢弃层</span></span><br><span class=\"line\">\t\tH2 = (nd.dot(H1, w2) + b2).relu()</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> autograd.is_training():</span><br><span class=\"line\">\t\t\tH2 = dropout(H2, drop_prob2)</span><br><span class=\"line\">\t\t\t<span class=\"comment\"># 在第二层全连接层后添加丢弃层</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> nd.dot(H2, w3) + b3</span><br><span class=\"line\">\t\t</span><br><span class=\"line\">\tnum_epochs, lr, batch_size = <span class=\"number\">5</span>, <span class=\"number\">0.5</span>, <span class=\"number\">256</span></span><br><span class=\"line\">\tloss = gloss.SoftmaxCrossEntropyLoss()</span><br><span class=\"line\">\ttrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class=\"line\">\td2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, params, lr)</span><br></pre></td></tr></tbody></table></figure>\n\n<h2 id=\"丢弃法简洁实现\"><a href=\"#丢弃法简洁实现\" class=\"headerlink\" title=\"丢弃法简洁实现\"></a>丢弃法简洁实现</h2><figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 数据收集如上</span></span><br><span class=\"line\">net = nn.Sequential()</span><br><span class=\"line\">net.add(nn.Dense(<span class=\"number\">256</span>, activation=<span class=\"string\">'relu'</span>)</span><br><span class=\"line\">\t\tnn.Dropout(drop_prob1)</span><br><span class=\"line\">\t\tnn.Dense(<span class=\"number\">256</span>, activation=<span class=\"string\">'relu'</span>)</span><br><span class=\"line\">\t\tnn.Dropout(drop_prob2)</span><br><span class=\"line\">\t\tnn.Dense(<span class=\"number\">10</span>))</span><br><span class=\"line\">net.initialize(init.Normal(sigma=<span class=\"number\">0.01</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">trainer = gluon.Trainer(net.collect_params(), <span class=\"string\">'sgd'</span>, {<span class=\"string\">'learning_rate'</span>: lr})</span><br><span class=\"line\">loss = gloss.SoftmaxCrossEntropyLoss()</span><br><span class=\"line\">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, <span class=\"literal\">None</span>, <span class=\"literal\">None</span>, trainer)</span><br></pre></td></tr></tbody></table></figure>",
            "tags": [
                "深度学习"
            ]
        },
        {
            "id": "https://chencc8.github.io/2022/03/07/shen-du-xue-xi-quan-chong-shuai-tui/",
            "url": "https://chencc8.github.io/2022/03/07/shen-du-xue-xi-quan-chong-shuai-tui/",
            "title": "深度学习-权重衰退",
            "date_published": "2022-03-06T19:58:30.000Z",
            "content_html": "<h1 id=\"深度学习-权重衰退\"><a href=\"#深度学习-权重衰退\" class=\"headerlink\" title=\"深度学习-权重衰退\"></a>深度学习-权重衰退</h1><ul>\n<li><p>使用均方范数作为<strong>硬性</strong>限制</p>\n<p>  通过限制参数值的选择范围来控制模型容量<br>  min l(<strong>W</strong>, b)  subject to ||<strong>w</strong>||2(平方) &lt;= θ</p>\n<p>  <strong>硬性限制是指每个w都必须小于θ</strong></p>\n<p>  通常不限制偏移b(限不限制都差不多)(整个数据在零点的偏移)</p>\n<p>  小的θ意味着更强的正则项</p>\n</li>\n<li><p>使用均方范数作为<strong>柔性</strong>限制</p>\n<p>  罚（penalty）：λ/2*||<strong>w</strong>||2</p>\n<p>  <strong>不是硬性要求，更平滑一点，不一定每一个w都在一定范围内</strong></p>\n<p>  超参数λ控制了正则项的重要程度</p>\n<p>  通过增加λ来减小模型复杂度</p>\n<p>  λ等于0时，惩罚项完全不起作用</p>\n<p>  λ越大，使得最优解越向原点靠近，即趋近于零</p>\n</li>\n<li><p>关于图的理解</p>\n<ul>\n<li><p>绿线：只优化损失函数的等高线</p>\n</li>\n<li><p>黄线：罚，以原点为中心的等高线</p>\n</li>\n<li><p>损失函数加正则项成为目标函数，目标函数最优解不是损失函数最优解。</p>\n</li>\n<li><p>加入罚之后原始解变得不是很优，对于罚来说很大，把最优解向原点拉动，l（损失）的值虽然变大，但罚的项变小。罚对原始最优解的拉动力较大，直到达到平衡点（增加值和减小值相等时）。总体来看，罚使得最优解向原点走。</p>\n</li>\n<li><p>首先要明确的是，w选择范围过大会使得模型可以拟合任意函数，使模型复杂度过大，导致过拟合。所以解决过拟合问题的思路应该是使w下降，方法之一是权重衰退。</p>\n</li>\n<li><p>所以过程大概是：过拟合 -&gt; w过大 -&gt; 为减小w，加入罚 -&gt; 最优解向原点拉动 -&gt; w绝对值减小 -&gt; 权重衰退 -&gt; 模型复杂度降低</p>\n</li>\n</ul>\n</li>\n</ul>\n<p>L2范数<strong>正则化</strong>又称权重衰退</p>\n<p>权重衰退通过<strong>惩罚绝对值较大的模型参数</strong>为需要学习的模型增加了限制，这可能对过拟合有效。</p>\n<h2 id=\"权重衰退简洁实现\"><a href=\"#权重衰退简洁实现\" class=\"headerlink\" title=\"权重衰退简洁实现\"></a>权重衰退简洁实现</h2><figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> d2lzh <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"><span class=\"keyword\">from</span> mxnet <span class=\"keyword\">import</span> autograd, gluon, init, nd</span><br><span class=\"line\"><span class=\"keyword\">from</span> mxnet.gluon <span class=\"keyword\">import</span> loss <span class=\"keyword\">as</span> gloss, data <span class=\"keyword\">as</span> gdata, nn</span><br><span class=\"line\"></span><br><span class=\"line\">n_train, n_test, num_inputs = <span class=\"number\">20</span>, <span class=\"number\">100</span>, <span class=\"number\">200</span></span><br><span class=\"line\">true_w, true_b = nd.ones((num_inputs, <span class=\"number\">1</span>)) * <span class=\"number\">0.01</span>, <span class=\"number\">0.05</span></span><br><span class=\"line\">features = nd.random.normal(shape=(n_train + n_test, num_inputs))</span><br><span class=\"line\">labels = nd.dot(features, true_w) + true_b</span><br><span class=\"line\">labels += nd.random.normal(scale=<span class=\"number\">0.01</span>, shape=labels.shape)</span><br><span class=\"line\">train_features, test_features = features[:n_train, :], features[n_train:, :]</span><br><span class=\"line\">train_labels, test_labels = labels[:n_train], labels[n_train:]</span><br><span class=\"line\"></span><br><span class=\"line\">batch_size, num_epochs, lr = <span class=\"number\">1</span>, <span class=\"number\">100</span>, <span class=\"number\">0.03</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">fit_and_plot_gluon</span>(<span class=\"params\">wd</span>):</span><br><span class=\"line\">\t<span class=\"comment\"># wd相当于</span></span><br><span class=\"line\">\tnet = nn.Sequential()</span><br><span class=\"line\">\tnet.add(nn.Dense(<span class=\"number\">1</span>))</span><br><span class=\"line\">\tnet.initialize(init.Normal(sigma=<span class=\"number\">1</span>))</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"comment\"># 直接在构造Trainer实例时通过wd参数来指定权重衰减超参数</span></span><br><span class=\"line\">\t<span class=\"comment\"># 默认下，gluon会对权重和偏差同时衰减</span></span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"comment\"># 仅对权重参数衰减，权重名称一般以weight结尾</span></span><br><span class=\"line\">\ttrainer_w = gluon.Trainer(net.collect_params(<span class=\"string\">'.*weight'</span>), <span class=\"string\">'sgd'</span>, {<span class=\"string\">'learning_rate'</span>: lr, <span class=\"string\">'wd'</span>: wd})</span><br><span class=\"line\">\t<span class=\"comment\"># 不对偏差参数衰减，偏差名称一般是以bias结尾</span></span><br><span class=\"line\">\ttrainer_b = gluon.Trainer(net.collect_params(<span class=\"string\">'.*bias'</span>), <span class=\"string\">'sgd'</span>, {<span class=\"string\">'learning_rate'</span>: lr})</span><br><span class=\"line\">\t</span><br><span class=\"line\">\ttrain_ls, teat_ls = [], []</span><br><span class=\"line\">\t<span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_epochs):</span><br><span class=\"line\">\t\t<span class=\"keyword\">for</span> X, y <span class=\"keyword\">in</span> train_iter:</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">with</span> autograd.record():</span><br><span class=\"line\">\t\t\t\tl = loss(net(X), y)</span><br><span class=\"line\">\t\t\tl.backward()</span><br><span class=\"line\">\t\t\t</span><br><span class=\"line\">\t\t\t<span class=\"comment\"># 对两个Trainer实例分别调用step函数，从而分别更新权重和偏差</span></span><br><span class=\"line\">\t\t\ttrainer_w.step(batch_size)</span><br><span class=\"line\">\t\t\ttrainer_b.step(batch_size)</span><br><span class=\"line\">\t\t</span><br><span class=\"line\">\t\ttrain_ls.append(loss(net(train_features), train_labels).mean().asscalar())</span><br><span class=\"line\">\t\ttest_ls.append(loss(net(test_features), test_labels).mean().asscalar())</span><br><span class=\"line\">\td2l.semilogy(<span class=\"built_in\">range</span>(<span class=\"number\">1</span>, num_epochs + <span class=\"number\">1</span>), train_ls, <span class=\"string\">'epoch'</span>, <span class=\"string\">'loss'</span>,</span><br><span class=\"line\">\t\t\t\t <span class=\"built_in\">range</span>(<span class=\"number\">1</span>, num_epochs + <span class=\"number\">1</span>), test_ls, [<span class=\"string\">'train'</span>, <span class=\"string\">'test'</span>])</span><br><span class=\"line\">\t<span class=\"built_in\">print</span>(<span class=\"string\">'L2 norm of w:'</span>, net[<span class=\"number\">0</span>].weight.data().norm().asscalar())</span><br></pre></td></tr></tbody></table></figure>",
            "tags": [
                "深度学习"
            ]
        },
        {
            "id": "https://chencc8.github.io/2022/03/07/shen-du-xue-xi-mo-xing-xuan-ze-guo-ni-he-he-qian-ni-he/",
            "url": "https://chencc8.github.io/2022/03/07/shen-du-xue-xi-mo-xing-xuan-ze-guo-ni-he-he-qian-ni-he/",
            "title": "深度学习-模型选择+过拟合和欠拟合",
            "date_published": "2022-03-06T19:58:30.000Z",
            "content_html": "<h1 id=\"深度学习-模型选择-过拟合和欠拟合\"><a href=\"#深度学习-模型选择-过拟合和欠拟合\" class=\"headerlink\" title=\"深度学习-模型选择+过拟合和欠拟合\"></a>深度学习-模型选择+过拟合和欠拟合</h1><h2 id=\"模型选择\"><a href=\"#模型选择\" class=\"headerlink\" title=\"模型选择\"></a>模型选择</h2><ul>\n<li><p>训练误差和泛化误差<br>  训练误差：模型在训练数据上的误差<br>  泛化误差：模型在新数据上的误差<br>  <strong>更看重泛化误差</strong></p>\n</li>\n<li><p>验证数据集和测试数据集<br>  验证数据集：一个用来评估模型好坏的数据集（<strong>不能和训练数据集混合！！！</strong>）<br>  测试数据集：<strong>只用一次</strong>的数据集<br>  <strong>验证数据集得到的精度不是真正代表在新数据集上的泛化能力</strong></p>\n</li>\n<li><p>K-则交叉验证</p>\n<ul>\n<li>在<strong>没有足够多的数据</strong>时使用（这是常态）</li>\n<li>算法：<ol>\n<li>将训练数据分割成K块</li>\n<li>for i = 1,…,K<br> 使用第i块作为验证数据集，其余作为训练数据集</li>\n<li>报告K个验证集误差的<strong>平均</strong></li>\n</ol>\n</li>\n<li>K常用5或10（很贵，看自己的能力）</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"过拟合和欠拟合\"><a href=\"#过拟合和欠拟合\" class=\"headerlink\" title=\"过拟合和欠拟合\"></a>过拟合和欠拟合</h2><table>\n<thead>\n<tr>\n<th align=\"center\">模型\\数据</th>\n<th align=\"center\">简单</th>\n<th align=\"center\">复杂</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">低</td>\n<td align=\"center\">正常</td>\n<td align=\"center\">欠拟合</td>\n</tr>\n<tr>\n<td align=\"center\">高</td>\n<td align=\"center\">过拟合</td>\n<td align=\"center\">正常</td>\n</tr>\n</tbody></table>\n<ul>\n<li><p><strong>模型容量</strong><br>  <strong>即拟合各种函数的能力</strong><br>  训练误差理论上可以到达零，即神经网络理论上可以记住所有数据<br>  核心：模型应先足够大（过拟合），通过各种手段控制模型容量，使得泛化误差下降</p>\n</li>\n<li><p><strong>估计模型容量</strong><br>  难以在不同的种类算法之间比较<br>  给定模型种类，有两个主要因素：<strong>参数个数</strong>，<strong>参数值的选择范围</strong></p>\n</li>\n<li><p><strong>VC维</strong>（了解）<br>  统计学习理论的一个核心思想<br>  支持N维输入的感知机的VC维是N+1<br>  一些多层感知机的VC维是O(Nlog2N)</p>\n<ul>\n<li>VC维的用处<ol>\n<li>提供为什么一个模型好的理论依据<br> 它可以衡量训练误差和泛化误差之间的间隔</li>\n<li>深度学习中很少使用<br> 衡量不是很准确<br> 计算深度学习模型的VC维很困难</li>\n</ol>\n</li>\n</ul>\n</li>\n<li><p><strong>数据复杂度</strong><br>  重要因素：样本个数，每个样本的元素个数，时间空间结构，多样性</p>\n</li>\n<li><p><strong>QA</strong></p>\n<ol>\n<li>Q：SVM和神经网络相比？<br>A：SVM通过kernel来匹配模型复杂度，SVM很难做到一百万个数据量。能调整的东西不多，可调性不强。<br> 神经网络本身是一种语言。不直观但可编程性很强的框架，可以做到很大数据集。</li>\n<li>Q：假设一个二分类问题，实际情况是1/9的比例，验证集两种类型的比例应该是？<br>A：验证数据集：两类1：1（数据不够多）</li>\n<li>Q：K-则交叉验证的目的是确定超参数吗？需要用这个超参数再训练一次吗？<br>A：3种情况：<br>（1）确定超参数后，在整个训练集上重新训练一遍。<br>（2）选定好的超参数（任一结果或最好结果），但代价是模型少看了一些训练集<br>（3）训练后测试时k个模型都预测一次，把K个预测结果做平均，能增加模型稳定性，但代价是k倍。</li>\n</ol>\n</li>\n</ul>\n",
            "tags": [
                "深度学习"
            ]
        },
        {
            "id": "https://chencc8.github.io/2022/03/06/shen-du-xue-xi-softmax-hui-gui/",
            "url": "https://chencc8.github.io/2022/03/06/shen-du-xue-xi-softmax-hui-gui/",
            "title": "深度学习-softmax回归",
            "date_published": "2022-03-06T11:28:30.000Z",
            "content_html": "<h1 id=\"深度学习-softmax回归\"><a href=\"#深度学习-softmax回归\" class=\"headerlink\" title=\"深度学习-softmax回归\"></a>深度学习-softmax回归</h1><ul>\n<li><p>概述<br>  softmax回归的输出单元为多个（离散值），为单层神经网络，也属于全连接层。</p>\n<p>  引入softmax运算使输出更适合离散值的预测和训练。</p>\n</li>\n<li><p>从回归到多类分类</p>\n<ul>\n<li>回归<ul>\n<li>单连续值输出</li>\n<li>自然区间R</li>\n<li>跟真实值的区别作为损失</li>\n</ul>\n</li>\n<li>分类<ul>\n<li>通常多个输出</li>\n<li>输出i是预测为第i类的置信度</li>\n<li>不关心实际值，而是对正确类别的置信度很大</li>\n<li>需要更置信的识别正确类（大余量）</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"softmax回归模型\"><a href=\"#softmax回归模型\" class=\"headerlink\" title=\"softmax回归模型\"></a>softmax回归模型</h2><ul>\n<li><p>输入和权重做线性叠加，softmax回归的<strong>输出值个数等于标签里的类别数</strong></p>\n</li>\n<li><p>可以直接使用输出值oi当作预测类别是i的置信度，并将值最大的输出所对应的类作为预测输出，即输出argmax oi。但输出层输出值的范围不确定且真实标签是离散值，这些离散值与不确定范围的输出值之间的范围难以衡量</p>\n</li>\n<li><p>故引用softmax运算将输出值变换成值为正且和为1的概率分布</p>\n</li>\n<li><p>softmax运算不改变预测类别</p>\n</li>\n</ul>\n</li>\n<li><p>交叉熵损失函数 </p>\n<ul>\n<li><p>真实标签也可以用类别分布表达，对于样本i，构造向量<strong>y</strong>i ∈R^q^， 使第<strong>y</strong>i个元素为1，其余为0（one-hot独热编码），使训练目标设为使预测概率分布<strong>y_hat</strong>(i)尽可能接近真实的标签概率分布<strong>y</strong>i</p>\n</li>\n<li><p>预测分类结果正确，并不需要预测概率完全等于标签概率。但平方损失过于严格</p>\n</li>\n<li><p>需要一个更适合衡量两个概率分布差异的测量函数，交叉熵是一个常用方法</p>\n</li>\n<li><p>交叉熵只关心对正确类别的预测概率</p>\n</li>\n<li><p>最小化交叉熵损失函数等价于最大化训练数据集所有标签类别的联合预测概率</p>\n</li>\n<li><p>其梯度是真实概率和预测概率的区别</p>\n</li>\n</ul>\n<h2 id=\"softmax回归简洁实现\"><a href=\"#softmax回归简洁实现\" class=\"headerlink\" title=\"softmax回归简洁实现\"></a>softmax回归简洁实现</h2></li>\n</ul>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> d2lzh <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"><span class=\"keyword\">from</span> mxnet <span class=\"keyword\">import</span> gluon, init</span><br><span class=\"line\"><span class=\"keyword\">from</span> mxnet.gluon <span class=\"keyword\">import</span> loss <span class=\"keyword\">as</span> gloss, nn</span><br><span class=\"line\">\t</span><br><span class=\"line\"><span class=\"comment\"># 读取数据集</span></span><br><span class=\"line\">batch_size = <span class=\"number\">256</span></span><br><span class=\"line\">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class=\"line\">\t</span><br><span class=\"line\">net = nn.Sequential()</span><br><span class=\"line\">net.add(nn.Dense(<span class=\"number\">10</span>))  <span class=\"comment\"># 单层神经网络，输出值个数为10类</span></span><br><span class=\"line\">net.initialize(init.Normal(sigma=<span class=\"number\">0.01</span>)</span><br><span class=\"line\">\t</span><br><span class=\"line\">loss = gloss.SoftmaxCrossEntropyLoss()  <span class=\"comment\"># 交叉熵函数</span></span><br><span class=\"line\">\t</span><br><span class=\"line\">trainer = gluon.Trainer(net.collect_params(), <span class=\"string\">'sgd'</span>, {<span class=\"string\">'learning_rate'</span>: <span class=\"number\">0.1</span>})</span><br><span class=\"line\">\t</span><br><span class=\"line\">num_epochs = <span class=\"number\">5</span>  <span class=\"comment\"># 迭代次数</span></span><br><span class=\"line\">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, <span class=\"literal\">None</span>, <span class=\"literal\">None</span>, trainer)</span><br></pre></td></tr></tbody></table></figure>\n\n<h2 id=\"损失函数\"><a href=\"#损失函数\" class=\"headerlink\" title=\"损失函数\"></a>损失函数</h2><ol>\n<li>L2 Loss（L2范数/平方损失）  梯度的绝对值在较远的地方下降较大</li>\n<li>L1 Loss  预测值和实际值距离较远时，梯度下降不变，稳定性，但靠近0时，会变得不稳定（不平滑性）</li>\n<li>Huber`s Robust Loss 结合上述两种损失函数的优点（稳定性+平滑性）</li>\n</ol>\n",
            "tags": [
                "深度学习"
            ]
        },
        {
            "id": "https://chencc8.github.io/2022/03/06/shen-du-xue-xi-duo-ceng-gan-zhi-ji/",
            "url": "https://chencc8.github.io/2022/03/06/shen-du-xue-xi-duo-ceng-gan-zhi-ji/",
            "title": "深度学习-多层感知机",
            "date_published": "2022-03-06T11:28:30.000Z",
            "content_html": "<h1 id=\"深度学习-多层感知机\"><a href=\"#深度学习-多层感知机\" class=\"headerlink\" title=\"深度学习-多层感知机\"></a>深度学习-多层感知机</h1><ul>\n<li>概述：多层神经网络</li>\n</ul>\n<h2 id=\"感知机\"><a href=\"#感知机\" class=\"headerlink\" title=\"感知机\"></a>感知机</h2><p>给入输入<strong>x</strong>，权重<strong>w</strong>，和偏移b，感知机输出：</p>\n<p>o = σ(&lt;**w**, **x**&gt; + b)</p>\n<p>x&gt;0时，σ(x) = 1</p>\n<p>x&lt;=0时，σ(x) = 0 / -1</p>\n<p><strong>单一元素输出可以做成二分类问题</strong></p>\n<p>softmax有n各类则有n个输出，属于多分类问题，感知机只输出一个元素，属于二分类问题</p>\n<p>线性回归输出的是实数，而感知机输出的是离散的类</p>\n<p>单层感知机只对<strong>线性可分</strong>的数据集有效</p>\n<ul>\n<li><p>收敛定理</p>\n<ul>\n<li><p>数据在半径r内</p>\n</li>\n<li><p>余量ρ分类两类</p>\n<p>  y(<strong>x</strong>T<strong>w</strong> + b) &gt;= ρ</p>\n<p>  即分类正确而且留有余量</p>\n</li>\n<li><p>对于||<strong>w</strong>||2 + b2 &lt;= 1，感知机保证在(r2 + 1)/ρ2步后收敛</p>\n</li>\n</ul>\n</li>\n<li><p>XOR问题难以解决，不能拟合XOR函数，它只能产生线性分割面。所以引入了激活函数。</p>\n</li>\n</ul>\n<h2 id=\"多层感知机\"><a href=\"#多层感知机\" class=\"headerlink\" title=\"多层感知机\"></a>多层感知机</h2><ul>\n<li><p><strong>隐藏层</strong>  </p>\n<ul>\n<li><p>位于输入层和输出层之间</p>\n</li>\n<li><p>多层感知机中隐藏层和输出层都是全连接层</p>\n</li>\n<li><p>隐藏层数和每层隐藏层大小是<strong>超参数</strong></p>\n</li>\n<li><p>神经网络越深，每层隐藏单元个数逐层减小，不断压缩，最下面一层可以大一点，减小的隐藏层个数过大会导致数据损失。</p>\n</li>\n<li><p>需加入激活函数，否则仍为单层神经网络</p>\n</li>\n</ul>\n</li>\n<li><p><strong>激活函数</strong>（常用ReLU函数） </p>\n<p>  避免层数的塌陷</p>\n<p>  全连接层只是对数据做仿射变换，而多个仿射变换的叠加仍然是放射变换，解决问题的一个方法是引入非线性变换，成为激活函数。</p>\n<ol>\n<li><p>ReLU函数</p>\n<p> ReLU(x) = max(x, 0)</p>\n<ul>\n<li>尽管输入为0时不可导，但此处导数可以取0。</li>\n</ul>\n</li>\n<li><p>sigmoid函数</p>\n<ul>\n<li><p>可以将元素的值变换到0~1之间</p>\n</li>\n<li><p>当输入接近0时，sigmoid函数接近线性变化</p>\n</li>\n<li><p>当输入为0时，sigmoid函数的导数最大值为0.25，偏离0时，导数接近0</p>\n</li>\n</ul>\n</li>\n<li><p>tanh函数</p>\n<ul>\n<li><p>可以将元素的值变换到-1~1之间</p>\n</li>\n<li><p>当输入接近0时，tanh函数接近线性变化</p>\n</li>\n<li><p>tanh函数在坐标系原点上对称</p>\n</li>\n<li><p>当输入为0时，tanh函数的导数最大值为1，偏离0时，导数接近0</p>\n</li>\n</ul>\n</li>\n</ol>\n</li>\n</ul>\n<ul>\n<li><p>多层感知机：含有至少一个隐藏层的由全连接层组成的神经网络，且每个输出层的输出通过激活函数进行变换</p>\n</li>\n<li><p>多层感知机的层数和各隐藏层中隐藏单元个数都是超参数</p>\n</li>\n<li><p>做多类分类的多层感知机可以使用softmax函数</p>\n</li>\n<li><p>主要步骤可以写成：</p>\n<ol>\n<li><p>定义一个神经网络</p>\n</li>\n<li><p>网络初始化</p>\n</li>\n<li><p>训练：</p>\n</li>\n</ol>\n<ul>\n<li><p>定义一个Trainer</p>\n</li>\n<li><p>传入数据及标签，数据大小batch_shape</p>\n</li>\n<li><p>前向传播计算loss</p>\n</li>\n<li><p>反向传播得到梯度</p>\n</li>\n<li><p>更新权重等参数</p>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"多层感知机简洁实现\"><a href=\"#多层感知机简洁实现\" class=\"headerlink\" title=\"多层感知机简洁实现\"></a>多层感知机简洁实现</h2><h3 id=\"mxnet版\"><a href=\"#mxnet版\" class=\"headerlink\" title=\"mxnet版\"></a>mxnet版</h3><figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> d2lzh <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"><span class=\"keyword\">from</span> mxnet <span class=\"keyword\">import</span> gluon, init</span><br><span class=\"line\"><span class=\"keyword\">from</span> mxnet.gluon <span class=\"keyword\">import</span> loss <span class=\"keyword\">as</span> gloss, nn</span><br><span class=\"line\">\t</span><br><span class=\"line\">net = nn.Sequential()</span><br><span class=\"line\">net.add(nn.Dense(<span class=\"number\">256</span>, activation=<span class=\"string\">'relu'</span>),</span><br><span class=\"line\">\t    nn.Dense(<span class=\"number\">10</span>))</span><br><span class=\"line\">\t    <span class=\"comment\"># 指定隐藏单元个数为256，并使用ReLU激活函数</span></span><br><span class=\"line\">\t    <span class=\"comment\"># 输出个数为10</span></span><br><span class=\"line\">\t</span><br><span class=\"line\">net.initialize(init.Normal(sigma=<span class=\"number\">0.01</span>))</span><br><span class=\"line\">\t</span><br><span class=\"line\">batch_size = <span class=\"number\">256</span></span><br><span class=\"line\">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class=\"line\">\t</span><br><span class=\"line\"><span class=\"comment\"># 计算交叉熵损失</span></span><br><span class=\"line\">loss = gloss.SoftmaxCrossEntropyLoss()</span><br><span class=\"line\">\t</span><br><span class=\"line\"><span class=\"comment\"># 优化算法，设定学习率</span></span><br><span class=\"line\">trainer = gluon.Trainer(net.collect_params(), <span class=\"string\">'sgd'</span>, {<span class=\"string\">'learning_rate'</span>: <span class=\"number\">0.5</span>})</span><br><span class=\"line\"></span><br><span class=\"line\">num_epochs = <span class=\"number\">5</span></span><br><span class=\"line\">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, <span class=\"literal\">None</span>, <span class=\"literal\">None</span>, trainer)</span><br></pre></td></tr></tbody></table></figure>\n\n<h3 id=\"PyTorch版\"><a href=\"#PyTorch版\" class=\"headerlink\" title=\"PyTorch版\"></a>PyTorch版</h3><figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\">net = nn.Sequential(nn.Flatten(), nn.Linear(<span class=\"number\">784</span>, <span class=\"number\">256</span>), nn.ReLU(), nn.Linear(<span class=\"number\">256</span>, <span class=\"number\">10</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">init_weights</span>(<span class=\"params\">m</span>):</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> <span class=\"built_in\">type</span>(m) == nn.Linear:</span><br><span class=\"line\">\t\tnn.init.normal_(m.weight, std=<span class=\"number\">0.01</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">net.apply(init_weights)</span><br><span class=\"line\"></span><br><span class=\"line\">batch_size, lr, num_epochs = <span class=\"number\">256</span>, <span class=\"number\">0.1</span>, <span class=\"number\">10</span></span><br><span class=\"line\">loss = nn.CrossEntropyLoss()</span><br><span class=\"line\">trainer = torch.optim.SGD(net.Parameters(), lr=lr)</span><br><span class=\"line\"></span><br><span class=\"line\">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class=\"line\"></span><br><span class=\"line\">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, <span class=\"literal\">None</span>, <span class=\"literal\">None</span>, trainer)</span><br></pre></td></tr></tbody></table></figure>\n\n<ul>\n<li><p>train_ch3函数</p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train_ch3</span>(<span class=\"params\">net, train_iter, test_iter, loss, num_epochs, batch_size, params=<span class=\"literal\">None</span>, lr=<span class=\"literal\">None</span>, trainer=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">    <span class=\"string\">\"\"\"Train and evaluate a model with CPU.\"\"\"</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_epochs):</span><br><span class=\"line\">        train_l_sum, train_acc_sum, n = <span class=\"number\">0.0</span>, <span class=\"number\">0.0</span>, <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> X, y <span class=\"keyword\">in</span> train_iter:</span><br><span class=\"line\">            <span class=\"keyword\">with</span> autograd.record():  <span class=\"comment\"># 记录梯度（正向传播记录loss）</span></span><br><span class=\"line\">                y_hat = net(X)  <span class=\"comment\"># 预测值</span></span><br><span class=\"line\">                l = loss(y_hat, y).<span class=\"built_in\">sum</span>() <span class=\"comment\"># 计算batch_size小批量样本交叉熵损失之和</span></span><br><span class=\"line\">              <span class=\"comment\"># 这里不除以是因为最后直接除以总样本数</span></span><br><span class=\"line\">            l.backward()  <span class=\"comment\"># 求梯度（反向传播求梯度）</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> trainer <span class=\"keyword\">is</span> <span class=\"literal\">None</span>:  <span class=\"comment\"># 从零实现</span></span><br><span class=\"line\">\t        \tsgd(params, lr, batch_size)  <span class=\"comment\"># 优化算法</span></span><br><span class=\"line\">\t        <span class=\"keyword\">else</span>:  <span class=\"comment\"># 简洁实现</span></span><br><span class=\"line\">\t        \ttrainer.step(batch_size)  <span class=\"comment\"># 一次迭代，每次迭代更新一次参数</span></span><br><span class=\"line\">\t        y = y.astype(<span class=\"string\">'float32'</span>)</span><br><span class=\"line\">\t        train_l_sum += l.asscalar()</span><br><span class=\"line\">\t        train_acc_sum += (y_hat.argmax(axis=<span class=\"number\">1</span>) == y).<span class=\"built_in\">sum</span>().asscalar()</span><br><span class=\"line\">\t <span class=\"comment\"># (y_hat.argmax(axis=1) == y)结果为由0和1组成的向量，相加为预测正确的数量。</span></span><br><span class=\"line\">\t <span class=\"comment\"># asscalar()函数使其变为标量</span></span><br><span class=\"line\">\t        n += y.size  <span class=\"comment\"># 总样本数</span></span><br><span class=\"line\">\t      test_acc = evaluate_accuracy(test_iter, net)</span><br><span class=\"line\">\t      <span class=\"built_in\">print</span>(<span class=\"string\">'epoch %d, loss %.4f, train acc %.3f, test acc %.3f'</span></span><br><span class=\"line\">\t            % (epoch + <span class=\"number\">1</span>, train_l_sum / n, train_acc_sum / n, test_acc))</span><br><span class=\"line\">\t<span class=\"comment\"># 除以n即计算平均</span></span><br></pre></td></tr></tbody></table></figure>\n</li>\n<li><p>训练器Trainer函数</p>\n</li>\n</ul>\n<ol>\n<li>注册优化器函数</li>\n<li>在with autograd.record():之后结合trainer.step(batch_size)更新权重。</li>\n</ol>\n",
            "tags": [
                "深度学习"
            ]
        },
        {
            "id": "https://chencc8.github.io/2022/03/06/shen-du-xue-xi-xian-xing-hui-gui/",
            "url": "https://chencc8.github.io/2022/03/06/shen-du-xue-xi-xian-xing-hui-gui/",
            "title": "深度学习-线性回归",
            "date_published": "2022-03-06T11:28:30.000Z",
            "content_html": "<h1 id=\"深度学习-线性回归\"><a href=\"#深度学习-线性回归\" class=\"headerlink\" title=\"深度学习-线性回归\"></a>深度学习-线性回归</h1><ul>\n<li><p>概述<br>  线性回归属于单层神经网络(一般有权重即为一层)，输出的是一个连续值，适用于回归问题。</p>\n</li>\n<li><p>字母含义<br>  x：特征（决定因素）<br>  <strong>w</strong>：权重(这个例子中是标量)<br>  b：偏差<br>  y：标签（实际值）<br>  y_hat(y^)：预测值</p>\n<h2 id=\"线性回归基本要素\"><a href=\"#线性回归基本要素\" class=\"headerlink\" title=\"线性回归基本要素\"></a>线性回归基本要素</h2></li>\n<li><p><strong>模型（房价预测）</strong><br>  y_hat = x1w1 + x2w2 + b</p>\n</li>\n<li><p><strong>模型训练</strong><br>  通过数据寻找特定的模型参数值，使模型在数据上的误差尽可能小。</p>\n</li>\n<li><p><strong>训练模型</strong><br>                 假设采集样本数为n，索引为i的样本的特征为x<del>1</del>^(i)^和x<del>2</del>^(i)^,标签为y^(i)^，对于索引为i的房屋，线性回归模型的房屋价格预测表达式为：<br>                                                     y_hat^(i)^ = x<del>1</del>^(i)^w<del>1</del> + x<del>2</del>^(i)^w<del>2</del> + b</p>\n</li>\n<li><p><strong>损失函数（一般使用平方函数）</strong> <span class=\"github-emoji\"><span>😳</span><img src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f633.png?v8\" aria-hidden=\"true\" onerror=\"this.parent.classList.add('github-emoji-fallback')\"></span><br>  通常选取一个非负数作为误差，数值越小误差越小。<br>  表达式：l^(i)^(w<del>1</del>, w<del>2</del>, b) =^1^/<del>2</del>(y_hat^(i)^ - y^(i)^)^2^    （<strong>^1^/<del>2</del>便于求导</strong>）</p>\n<p>  通常使用所有样本误差的平均来衡量模型预测的质量<br>  即 l(w<del>1</del>, w<del>2</del>, b) = ^1^/<del>n</del>   ∑<del>i=1</del>^n^  l^(i)^(w<del>1</del>, w<del>2</del>, b) =  ^1^/<del>n</del>   ∑<del>i=1</del>^n^   ^1^/<del>2</del>( x<del>1</del>^(i)^w<del>1</del> + x<del>2</del>^(i)^w<del>2</del> + b - y^(i)^)^2^</p>\n<p>  理想模型参数记为w<del>1</del>^<em>^, w<del>2</del>^</em>^, b^<em>^ ，即 w<del>1</del>^</em>^, w<del>2</del>^<em>^, b^</em>^ = argmin l(w<del>1</del>, w<del>2</del>, b)，使训练样本损失最小。</p>\n</li>\n<li><p><strong>优化算法</strong>  <span class=\"github-emoji\"><span>👿</span><img src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f47f.png?v8\" aria-hidden=\"true\" onerror=\"this.parent.classList.add('github-emoji-fallback')\"></span><br>  解析解：可以直接用公式表达的解。<br>  数值解：只能通过优化算法有限次迭代模型参数来尽可能降低损失函数的值。<br>  <strong>大多数深度学习模型没有解析解。</strong><br>  超参数：人为设定的参数，不需要通过训练模型学习。<br>  学习率：正数 </p>\n<ul>\n<li><strong>小批量随机梯度下降</strong>（sgd）  <span class=\"github-emoji\"><span>😵</span><img src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f635.png?v8\" aria-hidden=\"true\" onerror=\"this.parent.classList.add('github-emoji-fallback')\"></span><br>  算法：选取一组模型参数初始值（如随机选取），对参数进行多次迭代，使每次迭代都可能降低损失函数的值。每次迭代中，随机均匀采样一个由固定数目训练数据样本所组成的小批量，求小批量中数据样本的平均损失有关模型参数的导数（梯度）。</li>\n</ul>\n</li>\n<li><p><strong>模型预测</strong>（模型推断或模型测试）<br>  得到的不一定是最优解w<del>1</del>^*^, w<del>2</del>^<em>^, b^</em>^，而是对最优解的一个<strong>近似</strong>。</p>\n</li>\n<li><p><strong>神经网络图</strong><br>  使用神经网络图直观地表现模型结构，隐去了模型参数权重和偏差。<br>  输入个数也叫特征数或特征向量维度。<br>  输入层不涉及计算。<br>  输出层中的神经元与输入层中各个输入完全连接，这里的输出层又叫全连接层或稠密层。</p>\n<h2 id=\"线性回归从零实现\"><a href=\"#线性回归从零实现\" class=\"headerlink\" title=\"线性回归从零实现\"></a>线性回归从零实现</h2><p>  太懒了不想打了…   <span class=\"github-emoji\"><span>😶</span><img src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f636.png?v8\" aria-hidden=\"true\" onerror=\"this.parent.classList.add('github-emoji-fallback')\"></span></p>\n<h2 id=\"线性回归简洁实现\"><a href=\"#线性回归简洁实现\" class=\"headerlink\" title=\"线性回归简洁实现\"></a>线性回归简洁实现</h2>  <figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> mxnet <span class=\"keyword\">import</span> autograd, nd, gluon</span><br><span class=\"line\"><span class=\"keyword\">from</span> mxnet.gluon <span class=\"keyword\">import</span> data <span class=\"keyword\">as</span> gdata, loss <span class=\"keyword\">as</span> gloss, nn</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 生成数据集</span></span><br><span class=\"line\">num_inputs, num_examples = <span class=\"number\">2</span>, <span class=\"number\">1000</span></span><br><span class=\"line\">true_w = [<span class=\"number\">2</span>, -<span class=\"number\">3.4</span>]</span><br><span class=\"line\">true_b = <span class=\"number\">4.2</span></span><br><span class=\"line\">features = nd.random.normal(scale=<span class=\"number\">1</span>, shape=(num_examples, num_inputs))</span><br><span class=\"line\">labels = true_W[<span class=\"number\">0</span>] * features[:, <span class=\"number\">0</span>] + true_w[<span class=\"number\">1</span>] * features[:, <span class=\"number\">1</span>] + true_b</span><br><span class=\"line\"><span class=\"comment\"># 为使模型复杂加入噪音</span></span><br><span class=\"line\">labels += nd.random.normal(scale=<span class=\"number\">0.01</span>, shape=labels.shape)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 读取数据集</span></span><br><span class=\"line\">batch_size = <span class=\"number\">10</span>   <span class=\"comment\"># 批量大小</span></span><br><span class=\"line\"><span class=\"comment\"># 将训练数据的特征和标签组合</span></span><br><span class=\"line\">dataset = gdata.ArrayDataset(features, labels)</span><br><span class=\"line\">data_iter = gdata.DataLoader(dataset, batch_size, shuffle=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"comment\"># shuffle为True代表打乱顺序</span></span><br><span class=\"line\"></span><br><span class=\"line\">net = nn.Sequential()</span><br><span class=\"line\"><span class=\"comment\"># nn是neural networks（神经网络）的缩写</span></span><br><span class=\"line\"><span class=\"comment\"># net是Sequential（序列）实例。Sequential实例：看作一个串联各个层的容器</span></span><br><span class=\"line\">net.add(nn.Dense(<span class=\"number\">1</span>))</span><br><span class=\"line\"><span class=\"comment\"># 一个Dense表示一层，参数值为输出个数</span></span><br><span class=\"line\"><span class=\"comment\"># 在容器中依次加入层</span></span><br><span class=\"line\"><span class=\"comment\"># Gluon中无需指定每一层输入形状</span></span><br><span class=\"line\"></span><br><span class=\"line\">net.initialize(init.Normal(sigma=<span class=\"number\">0.01</span>))</span><br><span class=\"line\"><span class=\"comment\"># init模块提供模型初始化参数的各种方法</span></span><br><span class=\"line\">loss = gloss.L2Loss()</span><br><span class=\"line\"><span class=\"comment\"># 平方损失即L2范数损失</span></span><br><span class=\"line\">trainer = gluon.Trainer(net.collect_params(), <span class=\"string\">'sgd'</span>, {<span class=\"string\">'learing_rate'</span>: <span class=\"number\">0.03</span>})</span><br><span class=\"line\"><span class=\"comment\"># 创建Trainer实例，参数通过collect_params函数获取参数</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 训练参数</span></span><br><span class=\"line\">num_epochs = <span class=\"number\">3</span>  <span class=\"comment\"># 迭代次数</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">1</span>, nums_epochs + <span class=\"number\">1</span>):</span><br><span class=\"line\">\t<span class=\"keyword\">for</span> X, y <span class=\"keyword\">in</span> data_iter:</span><br><span class=\"line\">\t\t<span class=\"keyword\">with</span> autogard.record():</span><br><span class=\"line\">\t\t\tl = loss(net(X), y)</span><br><span class=\"line\">\t\tl.backward()  <span class=\"comment\"># 计算梯度</span></span><br><span class=\"line\">\t\ttrainer.step(batch_size)  <span class=\"comment\"># 表示一次迭代，每次迭代更新参数</span></span><br><span class=\"line\">\tl = loss(net(features), labels)</span><br><span class=\"line\">\t<span class=\"built_in\">print</span>(<span class=\"string\">'epoch %d, loss: %f'</span> %(epoch, l.mean().asnumpy()))</span><br></pre></td></tr></tbody></table></figure></li>\n</ul>\n<p>​    </p>\n",
            "tags": [
                "深度学习"
            ]
        }
    ]
}